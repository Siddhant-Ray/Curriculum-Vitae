@inproceedings{dietmuller2022new,
author = {Dietm√ºller, Alexander and \textbf{Ray}, \textbf{Siddhant} and Jacob, Romain and Vanbever, Laurent},
title = {A {N}ew {H}ope for {N}etwork {M}odel {G}eneralization},
year = {2022},
isbn = {9781450398992},
url = {https://doi.org/10.1145/3563766.3564104},
doi = {10.1145/3563766.3564104},
abstract = {Generalizing machine learning (ML) models for network traffic dynamics tends to be considered a lost cause. Hence for every new task, we design new models and train them on model-specific datasets closely mimicking the deployment environments. Yet, an ML architecture called Transformer has enabled previously unimaginable generalization in other domains. Nowadays, one can download a model pre-trained on massive datasets and only fine-tune it for a specific task and context with comparatively little time and data. These fine-tuned models are now state-of-the-art for many benchmarks.We believe this progress could translate to networking and propose a Network Traffic Transformer (NTT), a transformer adapted to learn network dynamics from packet traces. Our initial results are promising: NTT seems able to generalize to new prediction tasks and environments. This study suggests there is still hope for generalization through future research.},
booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
keywords = {packet-level modeling, transformer},
location = {Austin, Texas},
}

@article{ray2020machine,
  title={Machine {L}earning based {C}ell {A}ssociation for m{MTC} 5G {C}ommunication {N}etworks},
  author={\textbf{Ray}, \textbf{Siddhant} and Bhattacharyya, Budhaditya},
  journal={International Journal of Mobile Network Design and Innovation},
  volume={10},
  number={1},
  pages={10--16},
  year={2020},
  publisher={Inderscience Publishers (IEL)},
}

@inproceedings{siddhantnsdi,
  author = {\textbf{Ray},\textbf{Siddhant} and Jiang, Xi and Guo, Zhuohan and Jiang, Junchen and Feamster, Nick},
  title = {Transformer-based {P}redictions for {S}udden {N}etwork {C}hanges},
  year = {2024},
  isbn = {9781450398992},
  publisher = {USENIX Association},
  abstract = {Accurate predictions on sudden changes in network states are crucial for the integrity of real-time applications. Traditional heuristic models fall short, especially in tail cases, struggling to capture long-term network dependencies. Modelling network traces as time series sequences, we explore the use of a Transformer model architecture, known for its success in time series prediction, to model network trace dependencies, focusing on sudden change predictions. Our preliminary result on using a Transformer model for predicting one-way delay (OWD) shows observable improvement over the heuristic baseline in prediction loss. This suggests a promising direction for enhancing network predictability and optimizing resource utilization.},
  booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (Poster Session)},
  keywords = {packet-level modeling, transformer},
  location = {Santa Clara, CA},
  series = {NSDI '24},
  abbr={NSDI24},
  pdf={transformer_nsdi_poster.pdf},
}

@inproceedings{10.1145/3651890.3672274,
author = {Liu, Yuhan and Li, Hanchen and Cheng, Yihua and \textbf{Ray}, \textbf{Siddhant} and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and Jiang, Junchen},
title = {CacheGen: KV {C}ache {C}ompression and {S}treaming for {F}ast {L}arge {L}anguage {M}odel {S}erving},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3651890.3672274},
doi = {10.1145/3651890.3672274},
abstract = {As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays.CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5--4.3x and the total delay in fetching and processing contexts by 3.2--3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
keywords = {large language models, KV cache, compression},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}

@inproceedings{10.1145/3672198.3673797,
author = {Li, Hanchen and Liu, Yuhan and Cheng, Yihua and \textbf{Ray}, \textbf{Siddhant} and Du, Kuntai and Jiang, Junchen},
title = {Eloquent: {A} {M}ore {R}obust {T}ransmission {S}cheme for {LLM} {T}oken {S}treaming},
year = {2024},
isbn = {9798400707131},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3672198.3673797},
doi = {10.1145/3672198.3673797},
abstract = {To render each generated token in real-time for users, the Large Language Model (LLM) server generates tokens one by one and streams each token (or group of a few tokens) through the network to the user right after generation, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of later tokens even if the packets containing them arrive on time. With a measurement study, we show that current applications suffer from increased stalls under unstable networks.For this emerging token streaming problem in LLM Chatbots that differs from previous multimedia and text applications, we propose a novel transmission scheme, called Eloquent, which puts newly generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and, in the meantime, is independently rendered when received, avoiding the aforementioned stalls caused by missing packets. Through simulation under various networks, we show Eloquent reduces stall ratio (proportion of token rendering wait time) by 71.0\% compared to the retransmission method commonly used by real chatbot applications and by 31.6\% compared to the baseline packet duplication scheme. By tailoring Eloquent to fit the token-by-token generation of LLM, we enable the Chatbots to respond like an eloquent speaker for users to better enjoy pervasive AI.},
booktitle = {Proceedings of the 2024 SIGCOMM Workshop on Networks for AI Computing},
keywords = {Large Language Models, Real-Time Communication, Token Streaming},
location = {Sydney, NSW, Australia},
series = {NAIC '24}
}
