@inproceedings{dietmuller2022new,
author = {Dietm√ºller, Alexander and \textbf{Ray}, \textbf{Siddhant} and Jacob, Romain and Vanbever, Laurent},
title = {A New Hope for Network Model Generalization},
year = {2022},
isbn = {9781450398992},
url = {https://doi.org/10.1145/3563766.3564104},
doi = {10.1145/3563766.3564104},
abstract = {Generalizing machine learning (ML) models for network traffic dynamics tends to be considered a lost cause. Hence for every new task, we design new models and train them on model-specific datasets closely mimicking the deployment environments. Yet, an ML architecture called Transformer has enabled previously unimaginable generalization in other domains. Nowadays, one can download a model pre-trained on massive datasets and only fine-tune it for a specific task and context with comparatively little time and data. These fine-tuned models are now state-of-the-art for many benchmarks.We believe this progress could translate to networking and propose a Network Traffic Transformer (NTT), a transformer adapted to learn network dynamics from packet traces. Our initial results are promising: NTT seems able to generalize to new prediction tasks and environments. This study suggests there is still hope for generalization through future research.},
booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
keywords = {packet-level modeling, transformer},
location = {Austin, Texas},
}

@article{ray2020machine,
  title={Machine learning based cell association for mMTC 5G communication networks},
  author={\textbf{Ray}, \textbf{Siddhant} and Bhattacharyya, Budhaditya},
  journal={International Journal of Mobile Network Design and Innovation},
  volume={10},
  number={1},
  pages={10--16},
  year={2020},
  publisher={Inderscience Publishers (IEL)},
}

@inproceedings{siddhantnsdi,
  author = {\textbf{Ray},\textbf{Siddhant} and Jiang, Xi and Guo, Zhuohan and Jiang, Junchen and Feamster, Nick},
  title = {Transformer-based Predictions for Sudden Network Changes},
  year = {2024},
  isbn = {9781450398992},
  publisher = {USENIX Association},
  abstract = {Accurate predictions on sudden changes in network states are crucial for the integrity of real-time applications. Traditional heuristic models fall short, especially in tail cases, struggling to capture long-term network dependencies. Modelling network traces as time series sequences, we explore the use of a Transformer model architecture, known for its success in time series prediction, to model network trace dependencies, focusing on sudden change predictions. Our preliminary result on using a Transformer model for predicting one-way delay (OWD) shows observable improvement over the heuristic baseline in prediction loss. This suggests a promising direction for enhancing network predictability and optimizing resource utilization.},
  booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (Poster Session)},
  keywords = {packet-level modeling, transformer},
  location = {Santa Clara, CA},
  series = {NSDI '24},
  abbr={NSDI24},
  pdf={transformer_nsdi_poster.pdf},
}

@inproceedings{liu2024cachegen,
      title={CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving}, 
      author={Yuhan Liu and Hanchen Li and Yihua Cheng and \textbf{Siddhant Ray} and Yuyang Huang and Qizheng Zhang and Kuntai Du and Jiayi Yao and Shan Lu and Ganesh Ananthanarayanan and Michael Maire and Henry Hoffmann and Ari Holtzman and Junchen Jiang},
      publisher = {To appear in SIGCOMM '24},
      year={2024},
      eprint={2310.07240},
      archivePrefix={arXiv},
      primaryClass={cs.NI}
}
